---
title: "MSE Scenarios for short and long lived"
author: "Sarah Gaichas"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Setup for MSEs

Single area

Two surveys spring and fall

Testing alternating spring and fall

Baseline is all data all the time, 

CV = empirical value from stockeff across short and long lived species

Looks like ~0.2 is reasonable for redfish in ICELAND, higher here. 

Say its decent ~0.3

Eff = 100

Catch CV = 10%

Eff = 100

Project at F40% (75% F40% is HCR 1)

Assessment frequency 3 years

IID NAA

30 year spin up 30 year loop (10 assessment cycles)

F scenario for spin up? high followed by lower? Yes

Double Fmsy for 15 years and then Fmsy for 15 years

F.year1 = 0.1, Fhist = "F-H-L", Fmax = 2.5, Fmin = 1, changepoint = 0.5

sigma squared for NAA and recruitment?

cod assessment or redfish for these (Brian supplied redfish!)

Keep multinomial 

Single fleet logistic selectivity: shift one age to the right from maturity ogive

Different selectivity by spring and fall survey: mimic some long lived species

I think this script does this

```{r}

library(wham)
library(whamMSE)

main.dir = here::here()

# get median DOY for each seasonal survey
# from a local surdat pull 
# survdat_nobio <- readRDS(here::here("localhugefiles/survdat_nolength.rds"))
# fracyr <- survdat_nobio$survdat |> #file from April 2025
#   dplyr::mutate(DOY = lubridate::yday(EST_TOWDATE)) |>
#   dplyr::select(SEASON, DOY) |> dplyr::distinct() |>
#   dplyr::group_by(SEASON) |>
#   dplyr::summarise(medDOY = median(DOY, na.rm=T),
#                    fracyrmed = medDOY/365)
# fracyr
# # A tibble: 2 Ã— 3
#   SEASON medDOY fracyrmed
#   <chr>   <dbl>     <dbl>
# 1 FALL     296.     0.812
# 2 SPRING   108.     0.295

year_start  <- 1  # starting year in the burn-in period
year_end    <- 30  # end year in the burn-in period
MSE_years   <- 30     # number of years in the feedback loop
# Note: no need to include MSE_years in simulation-estimation 

info <- generate_basic_info(n_stocks = 1, 
                            n_regions = 1, 
                            n_indices = 2, # spring and fall
                            n_fleets = 1, 
                            n_seasons = 1, # don't need seasons
                            base.years = year_start:year_end,
                            n_feedback_years = MSE_years,
                            life_history = "long",
                            n_ages = 20,
                            # We want a bit higher F in the historical period
                            F_info = list(F.year1 = 0.1, Fhist = "F-H-L", Fmax = 2.5, Fmin = 1, change_time = 0.5, user_F = NULL),
                            catch_info = list(catch_cv = 0.1, catch_Neff = 100),
                            index_info = list(index_cv = 0.3, index_Neff = 100, fracyr_indices = c(0.295, 0.812), q = 0.2),
                            fracyr_spawn = 0.625) 

basic_info = info$basic_info # collect basic information
catch_info = info$catch_info # collect fleet catch information
index_info = info$index_info # collect survey information
F_info = info$F # collect fishing information

basic_info <- generate_NAA_where(basic_info = basic_info, move.type = 3) # no movement

move <- NULL

n_stocks  <- as.integer(basic_info['n_stocks'])
n_regions <- as.integer(basic_info['n_regions'])
n_fleets  <- as.integer(basic_info['n_fleets'])
n_indices <- as.integer(basic_info['n_indices'])
n_ages    <- as.integer(basic_info['n_ages'])

# Selectivity Configuration
# for logistic pars are a50 and 1/slope
# approximate from redfish model
fleet_pars <- c(9,1)
index_pars <- list(c(5,1), c(3,1))
sel <- list(model=rep("logistic",n_fleets+n_indices),
            initial_pars=c(rep(list(fleet_pars),n_fleets),
                           #rep(list(index_pars),n_indices)))
                           index_pars))

# redfish M is 0.05 but I'll leave this at 0.1 
# M Configuration
M <- list(model="constant",initial_means=array(0.1, dim = c(n_stocks,n_regions,n_ages)))

sigma      <- "rec+1"
re_cor     <- "iid"
ini.opt    <- "equilibrium" # option   <- c("age-specific-fe", "equilibrium")

# # Set para. for B-H function, not using
# alpha <- 12
# beta  <- 1.5e-4

# Set sigma for NAA
NAA_sig <- 0.2
sigma_vals = array(NAA_sig, dim = c(n_stocks, n_regions, n_ages)) # n_stocks x n_regions x n_ages"

# Set initial NAA for each stock
log_N1  <- rep(10, n_stocks) # Create difference between stocks
N1_pars <- generate_ini_N1(log_N1,basic_info,ini.opt)

NAA_re <- list(N1_model=rep(ini.opt,n_stocks),
               sigma=rep(sigma,n_stocks),
               cor=rep(re_cor,n_stocks),
               recruit_model = 2,
               #recruit_pars = rep(list(c(alpha,beta)),n_stocks), # assume same B-H s-r functions for all stocks
               sigma_vals = sigma_vals,
               N1_pars = N1_pars)#,
               #NAA_where = basic_info$NAA_where)

# recruit_model = 1: estimating annual recruitments as fixed effects or a random walk if NAA_re$sigma specified
# recruit_model = 2: estimating a mean recruitment with annual recruitments as random effects
# recruit_model = 3: Beverton-Holt stock-recruitment with annual recruitments as random effects
# recruit_model = 4: Ricker stock-recruitment with annual recruitments as random effects

# 1. recruit_pars: a list (length = n_stocks) of vectors of initial parameters for recruitment model. 
# If $recruit_model is 3 (B-H) or 4 (Ricker), parameters are "alpha" and "beta".

# 2. sigma_vals: Initial standard deviation values to use for the NAA deviations. Values are not used if recruit_model = 1 
# If sigma="rec": must be a list (length = n_stocks) of single values
# If sigma="rec+1": a list (length = n_stocks) of 2 values must be specified. First is for the first age class (recruits), second is for all other ages.


input <- prepare_wham_input(basic_info = basic_info, 
                            selectivity = sel, 
                            M = M, 
                            NAA_re = NAA_re, 
                            move = move,
                            catch_info = catch_info, 
                            index_info = index_info, 
                            F = F_info)

# IMPORTANT!#
# This appears to be due to the initial F value for the Newton iterations to find F40. The default value is too high for long-lived fish
input$data$FXSPR_init[] <- 0.01 # change to a low value

# -------------------
# IMPORTANT!#
# -------------------
# Change initial numbers at age in the input for OM
# ini.NAA <- matrix(NA, n_ages, n_stocks)
# ini.NAA[,1] <- c(20:1) # Use actually initial numbers at age from assessment model?
# 
# input$par$log_N1[] = 0
# for (i in 1:n_regions) {
#   input$par$log_N1[i,i,] = log(ini.NAA[,i])
# }

random = input$random # check what processes are random effects
input$random = NULL # so inner optimization won't change simulated RE
om <- fit_wham(input, do.fit = F, do.brps = T, MakeADFun.silent = TRUE)
# Note: do.fit must be FALSE (no modeling fitting yet)

om_with_data <- update_om_fn(om, seed = 123, random = random)

assess.interval <- 3 # 
base.years      <- year_start:year_end # Burn-in period
first.year      <- head(base.years,1)
terminal.year   <- tail(base.years,1)
assess.years    <- seq(terminal.year, tail(om$years,1)-assess.interval,by = assess.interval)

mods <- list() # Create a list to save MSE outputs

NAA_re_em <- list(N1_model="equilibrium",sigma="rec+1",cor="iid")
# 
# M_em <- list(model="constant",initial_means=array(0.1, dim = c(n_stocks,n_regions,n_ages))) # this is misspecified M

mods[[1]] = loop_through_fn(om = om_with_data,
                            em_info = info, 
                            random = random,
                            M_em = M, # use OM M
                            sel_em = sel, # use OM sel
                            NAA_re_em = NAA_re_em, # use rec assumed random around the mean instead, help runtime (est B-H is difficult)
                            move_em = NULL,
                            age_comp_em = "multinomial",
                            # Here is the correct code: separate.em = FALSE also works for one-area model
                            em.opt = list(separate.em = FALSE, separate.em.type = 1, 
                                          do.move = FALSE, est.move = FALSE),
                            assess_years = assess.years, 
                            assess_interval = assess.interval, 
                            base_years = base.years,
                            year.use = 30, # number of years of data you want to use in the assessment model
                            add.years = TRUE, # extends assessment time series instead of moving window of year.use years
                            seed = 123,
                            save.sdrep = FALSE, 
                            save.last.em = TRUE,
                            FXSPR_init = 0.01) # IMPORTANT!


```


## Degraded data quality scenarios: 

Alternate spring and fall, double CV and half Eff (50)

Fishery data, double CV (0.2) and half Eff (50)

```{r}
agg_index_sigma = input$data$agg_index_sigma
agg_index_sigma[31:60,] = 0.6 # Increase CV for both survey indices in the feedback period
index_Neff = input$data$index_Neff
index_Neff[31:60,] = 50 # Decrease ESS for both survey indices in the feedback period

#alternate years fall and spring surveys
remove_agg = TRUE # remove a aggregate index for some years
remove_agg_pointer = c(1,2) # both
remove_agg_years = matrix(data=c(seq(31,60,2), seq(32,60,2)), nrow=15, ncol=2) #alternating years by survey
remove_paa = TRUE # Also remove age comp for that index 
remove_paa_pointer = c(1,2) # both
remove_paa_years = matrix(data=c(seq(31,60,2), seq(32,60,2)), nrow=15, ncol=2) #alternating years by survey

input <- update_input_index_info(input, agg_index_sigma, index_Neff,
                                 remove_agg, remove_agg_pointer, remove_agg_years,
                                 remove_paa, remove_paa_pointer, remove_paa_years) # Update input file

agg_catch_sigma = input$data$agg_catch_sigma
agg_catch_sigma[31:60,] = 0.2 #double catch CV in the feedback period
catch_Neff = input$data$catch_Neff
catch_Neff[31:60] = 50

input <- update_input_catch_info(input, agg_catch_sigma, catch_Neff)

```

Generate new om and dataset, keep same assessment interval as above

```{r}
om <- fit_wham(input, do.fit = F, do.brps = T, MakeADFun.silent = TRUE)
om_with_data <- update_om_fn(om, seed = 123, random = random)
```

Test new EM

```{r}

mods[[2]] = loop_through_fn(om = om_with_data,
                            em_info = info, 
                            random = random,
                            M_em = M, # use OM M
                            sel_em = sel, # use OM sel
                            NAA_re_em = NAA_re_em, # use rec assumed random around the mean instead, help runtime (est B-H is difficult)
                            move_em = NULL,
                            age_comp_em = "multinomial",
                            # Here is the correct code: separate.em = FALSE also works for one-area model
                            em.opt = list(separate.em = FALSE, separate.em.type = 1, 
                                          do.move = FALSE, est.move = FALSE),
                            assess_years = assess.years, 
                            assess_interval = assess.interval, 
                            base_years = base.years,
                            year.use = 30, # number of years of data you want to use in the assessment model
                            add.years = TRUE, # extends assessment time series instead of moving window of year.use years
                            seed = 123,
                            save.sdrep = FALSE, 
                            save.last.em = TRUE,
                            FXSPR_init = 0.01) # IMPORTANT!

```


Visualize 2 mods (modified from https://lichengxue.github.io/whamMSE/03.Management-Strategy-Evaluation.html#12_Compare_model_performance)

This compares the base case,good surveys in the future (1) with degraded data, alternating year seasonal surveys with lower quality sampling (catch too) in the future (2)

```{r}

par(mfrow = c(1,2))
SSB_s1 <- lapply(mods, function(mod) mod$om$rep$SSB[,1])
plot(SSB_s1[[1]], type = "l", col = "blue", ylab = "SSB", xlab = "Year", ylim=c(0,100000), main = "OM SSB") 
colors <- c("red","green","purple","orange")
for (i in length(SSB_s1)) {
  lines(SSB_s1[[i]], col = colors[i-1],lty = i)
}
legend("topleft",legend = paste0("EM ", 1:2), col = c("blue",colors),lty=1:5, cex=0.8)

Catch_s1 <- lapply(mods, function(mod) mod$om$rep$pred_catch[,1])
plot(Catch_s1[[1]], type = "l", col = "blue", ylab = "Catch", xlab = "Year", ylim=c(0,30000), main = "OM Catch")
colors <- c("red","green","purple","orange")
for (i in length(Catch_s1)) {
  lines(Catch_s1[[i]], col = colors[i-1],lty = i)
}

```

## Empirical management procedure

Index based method:  Ismooth averaging most recent 3 years of whatever surveys (rescaled to mean of each using Ismooth package)

Ismooth package is here https://github.com/cmlegault/PlanBsmooth

Do this later...

For now, do

100 replicates for each, try the parallel processing

* Base scenario 
* Alternating surveys and less age comps with 3 year assessments
* Empirical management procedures (later)





